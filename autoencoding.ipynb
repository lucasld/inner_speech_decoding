{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_preprocessing as dp\n",
    "import utilities\n",
    "import models.training as train\n",
    "from models.autoencoder import ConvolutionalAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset to train the Autoencoder\n",
    "\n",
    "```python\n",
    "data, events = dp.load_data()\n",
    "# filter out data and events correponding to the inner speech condition\n",
    "data, events = dp.choose_condition(data, events, 'inner speech')\n",
    "data = data.astype(np.float16) * 100_000  # NORMALIZATION ONLY POSSIBLE BY MULTIPLYING HERE?\n",
    "norm_data = dp.normalization(data)\n",
    "\n",
    "autoencoder_dataset = tf.data.Dataset.from_tensor_slices((norm_data, norm_data))\n",
    "autoencoder_dataset = dp.preprocessing_pipeline(\n",
    "    autoencoder_dataset,\n",
    "    functions = [dp.filter_interval_tensor,\n",
    "                 lambda sample: (tf.reshape(sample[0], (128, 640, 1)),\n",
    "                                 tf.reshape(sample[0], (128, 640, 1)))\n",
    "                ],\n",
    "    args = [[[1, 3.5], 256, [0, 1]], []],\n",
    "    batch_size = 12\n",
    ")\n",
    "\n",
    "# save dataset so that we can just load the preprocessed version next time\n",
    "tf.data.experimental.save(autoencoder_dataset, 'dataset/preprocessed/autoencoder_ds')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open: dataset/preprocessed/autoencoder_ds\\dataset_spec.pb : Das System kann den angegebenen Pfad nicht finden.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1536/1961672601.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# load dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'dataset/preprocessed/autoencoder_ds'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m autoencoder_datasets = dp.split_dataset(dataset,\n\u001B[0;32m      5\u001B[0m                                         splits={'train':0.9,\n",
      "\u001B[1;32m~\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\io.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(path, element_spec, compression, reader_func)\u001B[0m\n\u001B[0;32m    225\u001B[0m   \"\"\"\n\u001B[0;32m    226\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 227\u001B[1;33m   return _LoadDataset(\n\u001B[0m\u001B[0;32m    228\u001B[0m       \u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m       \u001B[0melement_spec\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0melement_spec\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\io.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, path, element_spec, compression, reader_func)\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0melement_spec\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mgfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDATASET_SPEC_FILENAME\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 135\u001B[1;33m         \u001B[0mencoded_spec\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    136\u001B[0m       \u001B[0mstruct_pb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnested_structure_coder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstruct_pb2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStructuredValue\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m       \u001B[0mstruct_pb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mParseFromString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded_spec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m    115\u001B[0m       \u001B[0mstring\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstring\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mregular\u001B[0m\u001B[1;33m)\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     \"\"\"\n\u001B[1;32m--> 117\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_preread_check\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m       \u001B[0mlength\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtell\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001B[0m in \u001B[0;36m_preread_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     77\u001B[0m         raise errors.PermissionDeniedError(None, None,\n\u001B[0;32m     78\u001B[0m                                            \"File isn't open for reading\")\n\u001B[1;32m---> 79\u001B[1;33m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001B[0m\u001B[0;32m     80\u001B[0m           compat.path_to_str(self.__name), 1024 * 512)\n\u001B[0;32m     81\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotFoundError\u001B[0m: NewRandomAccessFile failed to Create/Open: dataset/preprocessed/autoencoder_ds\\dataset_spec.pb : Das System kann den angegebenen Pfad nicht finden.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = tf.data.experimental.load('dataset/preprocessed/autoencoder_ds')\n",
    "\n",
    "autoencoder_datasets = dp.split_dataset(dataset,\n",
    "                                        splits={'train':0.9,\n",
    "                                                'test':0.05,\n",
    "                                                'valid':0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = ConvolutionalAutoencoder()\n",
    "autoencoder.build((None, 128,640,1))\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "for _input, target in autoencoder_datasets['train'].take(1):\n",
    "    out = autoencoder(_input)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(target[0][:,:,0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(out[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# Initialize the loss-function\n",
    "loss_func = tf.keras.losses.MeanSquaredError()\n",
    "# Initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Initialize Train-Object\n",
    "trainer = train.Trainer(autoencoder, autoencoder_datasets, optimizer, loss_func)\n",
    "# Initialize Plotting-Object\n",
    "grapher = utilities.TrainingGrapher(2,1, supxlabel='Epochs', axs_xlabels=[['train loss', 'test loss']])\n",
    "\n",
    "for epoch in range(30):\n",
    "    print(epoch, end='\\r')  \n",
    "    trainer.train_epoch()\n",
    "    grapher.update([trainer.losses['train'], trainer.losses['test']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "grapher.fig.set_size_inches(15, 8)\n",
    "grapher.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in autoencoder_datasets['valid'].take(1):\n",
    "    out_enc = autoencoder.encoder(i)\n",
    "    out_dec = autoencoder.decoder(out_enc)\n",
    "    f, axs = plt.subplots(1,3,figsize=(15,15))\n",
    "    axs[0].imshow(t[0][:,:,0])\n",
    "    axs[1].imshow(out_dec[0][:,:,0])\n",
    "    # normalize each channel\n",
    "    norm_enc = out_enc[0] / np.max(out_enc[0], (0, 1))[np.newaxis, np.newaxis, :]\n",
    "    axs[2].imshow(norm_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "autoencoder.save('./models/saved_models/autoencoder_30Epochs')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca8d0a0b18e5ead98978c779fcc98c2ce0d58c7314eef98ffee0524dd190160b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}